-- Step 1: Backup current processed_sensor_readings (optional safety measure)
-- We'll overwrite processed_sensor_readings with data from processed_sensor_readings_duplicate

-- Step 2: Clear processed_sensor_readings and copy all data from duplicate
TRUNCATE TABLE processed_sensor_readings RESTART IDENTITY CASCADE;

INSERT INTO processed_sensor_readings (
    original_id, recorded_at, temperature, humidity, pressure, gas_resistance,
    pm1_0, pm2_5, pm10, accel_x, accel_y, accel_z, accel_magnitude,
    gyro_x, gyro_y, gyro_z, gyro_magnitude, anomaly_score,
    predicted_failure_probability, quality_score, maintenance_recommendation,
    processing_version, location, created_at, processed_at, updated_at
)
SELECT 
    original_id, recorded_at, temperature, humidity, pressure, gas_resistance,
    pm1_0, pm2_5, pm10, accel_x, accel_y, accel_z, accel_magnitude,
    gyro_x, gyro_y, gyro_z, gyro_magnitude, anomaly_score,
    predicted_failure_probability, quality_score, maintenance_recommendation,
    processing_version, location, created_at, processed_at, updated_at
FROM processed_sensor_readings_duplicate;

-- Step 3: Create the new mock_sensor_dataset table structure
CREATE TABLE mock_sensor_dataset (
    id bigint GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    original_id integer NOT NULL,
    recorded_at timestamp with time zone NOT NULL,
    temperature real,
    humidity real,
    pressure real,
    gas_resistance real,
    pm1_0 integer,
    pm2_5 integer,
    pm10 integer,
    accel_x real,
    accel_y real,
    accel_z real,
    accel_magnitude real,
    gyro_x real,
    gyro_y real,
    gyro_z real,
    gyro_magnitude real,
    anomaly_score real DEFAULT 0,
    predicted_failure_probability real DEFAULT 0,
    quality_score integer DEFAULT 100,
    processed_at timestamp with time zone DEFAULT now(),
    updated_at timestamp with time zone DEFAULT now(),
    maintenance_recommendation text,
    processing_version text DEFAULT 'v1.0',
    location text DEFAULT 'hangar_01',
    created_at timestamp with time zone DEFAULT now(),
    is_mock_data boolean DEFAULT false
);

-- Step 4: Copy existing data from duplicate to new table
INSERT INTO mock_sensor_dataset (
    original_id, recorded_at, temperature, humidity, pressure, gas_resistance,
    pm1_0, pm2_5, pm10, accel_x, accel_y, accel_z, accel_magnitude,
    gyro_x, gyro_y, gyro_z, gyro_magnitude, anomaly_score,
    predicted_failure_probability, quality_score, maintenance_recommendation,
    processing_version, location, created_at, processed_at, updated_at,
    is_mock_data
)
SELECT 
    original_id, recorded_at, temperature, humidity, pressure, gas_resistance,
    pm1_0, pm2_5, pm10, accel_x, accel_y, accel_z, accel_magnitude,
    gyro_x, gyro_y, gyro_z, gyro_magnitude, anomaly_score,
    predicted_failure_probability, quality_score, maintenance_recommendation,
    processing_version, location, created_at, processed_at, updated_at,
    false as is_mock_data
FROM processed_sensor_readings_duplicate;

-- Step 5: Drop the old duplicate table
DROP TABLE processed_sensor_readings_duplicate;

-- Step 6: Enable RLS on the new table
ALTER TABLE mock_sensor_dataset ENABLE ROW LEVEL SECURITY;

-- Step 7: Create RLS policies for the new table
CREATE POLICY "Users can view all mock sensor data" 
ON mock_sensor_dataset 
FOR SELECT 
USING (true);

CREATE POLICY "Authenticated users can insert mock sensor data" 
ON mock_sensor_dataset 
FOR INSERT 
WITH CHECK (auth.uid() IS NOT NULL);

CREATE POLICY "Authenticated users can update mock sensor data" 
ON mock_sensor_dataset 
FOR UPDATE 
USING (auth.uid() IS NOT NULL);